{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os,json,pickle,collections,time\n",
    "import numpy as np\n",
    "from utils import *\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"]=\"2\"\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "\n",
    "config = tf.ConfigProto()  \n",
    "config.gpu_options.allow_growth=True  \n",
    "sess = tf.Session(config=config)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.layers import *\n",
    "from layer import GAT\n",
    "from keras import activations, constraints, initializers, regularizers\n",
    "from keras import backend as K\n",
    "from keras.layers import Layer, Dropout, LeakyReLU\n",
    "import tensorflow as tf\n",
    "\n",
    "class TokenEmbedding(keras.layers.Embedding):\n",
    "    \"\"\"Embedding layer with weights returned.\"\"\"\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return self.input_dim, self.output_dim\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return None\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return K.identity(self.embeddings)\n",
    "    \n",
    "def get_attr_model(node_size,prop_size,node_hidden,batch_size,\n",
    "                   n_attn_heads = 2,dropout_rate = 0.3,gamma = 3,lr = 0.005,activation='relu'):\n",
    "\n",
    "    ent_p = Input(shape=(None,prop_size))\n",
    "    ent_input = Lambda(lambda x: K.cast(K.squeeze(x,axis=0), dtype='float32'))(ent_p)\n",
    "    ent_hidden = Dense(node_hidden,activation=activation)(ent_input)\n",
    "    ent_hidden = BatchNormalization()(ent_hidden)\n",
    "    output = Dense(node_hidden,activation=activation)(ent_hidden)\n",
    "    output = Dropout(dropout_rate)(output)\n",
    "    alignment_input = Input(shape=(None,4))\n",
    "    find = Lambda(lambda x:K.gather(reference=x[0],indices=K.cast(K.squeeze(x[1],axis=0), 'int32')))([output,alignment_input])\n",
    "\n",
    "    def loss_function(tensor):\n",
    "        def dis(ll,rr):\n",
    "            return K.sum(K.abs(ll-rr),axis=-1,keepdims=True)\n",
    "        l,r,fl,fr = [tensor[:,0,:],tensor[:,1,:],tensor[:,2,:],tensor[:,3,:]]\n",
    "        loss = K.relu(gamma + dis(l,r) - dis(l,fr)) + K.relu(gamma + dis(l,r) - dis(fl,r))\n",
    "        return tf.reduce_sum(loss,keep_dims=True) / (batch_size)\n",
    "    loss = Lambda(loss_function)(find)\n",
    "    \n",
    "    inputs = [ent_p]\n",
    "    train_model = keras.Model(inputs = inputs + [alignment_input],outputs = loss)\n",
    "    train_model.compile(loss=lambda y_true,y_pred: y_pred,optimizer=keras.optimizers.adam(lr=lr))\n",
    "\n",
    "    feature_model = keras.Model(inputs = inputs,outputs = [output])\n",
    "    return train_model,feature_model\n",
    "\n",
    "def get_struc_model(node_size,rel_size,node_hidden,rel_hidden,triple_size,batch_size,\n",
    "                    n_attn_heads = 2,dropout_rate = 0.3,gamma = 3,lr = 0.005,depth = 2):\n",
    "    \n",
    "    adj_input = Input(shape=(None,2))\n",
    "    rel_adj = Input(shape=(None,2))\n",
    "    ent_adj = Input(shape=(None,2))\n",
    "    \n",
    "    org_feature = TokenEmbedding(node_size,node_hidden,trainable = True)(adj_input) \n",
    "    rel_feature = TokenEmbedding(rel_size,rel_hidden,trainable = True)(adj_input)\n",
    "    gat_in = [org_feature,rel_feature,adj_input,rel_adj,ent_adj]\n",
    "\n",
    "    \n",
    "    ent_feature = GAT(node_size,activation='relu',\n",
    "                               rel_size = rel_size,\n",
    "                               depth = depth,\n",
    "                               attn_heads=n_attn_heads,\n",
    "                               triple_size = triple_size,\n",
    "                               attn_heads_reduction='average',   \n",
    "                               dropout_rate=dropout_rate)(gat_in)\n",
    "    ent_feature = Dropout(dropout_rate)(ent_feature)    \n",
    "    \n",
    "    alignment_input = Input(shape=(None,4))\n",
    "    find = Lambda(lambda x:K.gather(reference=x[0],indices=K.cast(K.squeeze(x[1],axis=0), 'int32')))([ent_feature,alignment_input])\n",
    "\n",
    "    def loss_function(tensor):\n",
    "        def dis(ll,rr):\n",
    "            return K.sum(K.abs(ll-rr),axis=-1,keepdims=True)\n",
    "        l,r,fl,fr = [tensor[:,0,:],tensor[:,1,:],tensor[:,2,:],tensor[:,3,:]]\n",
    "        loss = K.relu(gamma + dis(l,r) - dis(l,fr)) + K.relu(gamma + dis(l,r) - dis(fl,r))\n",
    "        return tf.reduce_sum(loss,keep_dims=True) / (batch_size)\n",
    "    loss = Lambda(loss_function)(find)\n",
    "    \n",
    "    inputs = [adj_input,rel_adj,ent_adj]\n",
    "    train_model = keras.Model(inputs = inputs + [alignment_input],outputs = loss)\n",
    "    train_model.compile(loss=lambda y_true,y_pred: y_pred,optimizer=keras.optimizers.adam(lr=lr))\n",
    "\n",
    "    feature_model = keras.Model(inputs = inputs,outputs = [ent_feature])\n",
    "    return train_model,feature_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_attr(ent_prop_matrix,train_pair,dim=100,lr=0.005):\n",
    "   \n",
    "    model,get_emb = get_attr_model(lr=0.005,dropout_rate=0.3,node_size=node_size,\n",
    "                              prop_size=prop_size,batch_size = batch_size,gamma = 3,node_hidden=dim)\n",
    "    #model.summary()\n",
    "    losses,ave_loss =[],[]\n",
    "    batch = get_train_set(train_pair,ents1,ents2,batch_size=batch_size)\n",
    "    for epoch in range(2000):\n",
    "        train_set = next(batch)\n",
    "        inputs = [ent_prop_matrix,train_set]\n",
    "        inputs = [np.expand_dims(item,axis=0) for item in inputs]\n",
    "\n",
    "        loss = model.train_on_batch(inputs, np.array([0]))\n",
    "        losses.append(loss)\n",
    "        ave_loss.append(np.mean(losses[-100:]))\n",
    "        \n",
    "        if epoch%100 == 0:\n",
    "            print('\\tEpoch %d \\tloss=%.5f \\tave_loss=%.4f'%(epoch,loss,ave_loss[-1]))\n",
    "\n",
    "        if epoch > 500 and loss<ave_loss[-1] and ave_loss[-1] > ave_loss[-100]:\n",
    "            print('\\tEpoch %d \\tloss=%.5f \\tave_loss=%.4f'%(epoch,loss,ave_loss[-1]))\n",
    "            break # Early stop.\n",
    "\n",
    "    vec = get_emb.predict_on_batch(inputs[:-1])\n",
    "    return vec\n",
    "        \n",
    "def model_struc(adj_matrix,rel_matrix,ent_matrix,train_pair,dim=100,lr=0.005):\n",
    "    \n",
    "    model,get_emb = get_struc_model(lr=0.005,dropout_rate=0.3,node_size=node_size,rel_size=rel_size,n_attn_heads = 2,\n",
    "                                      depth=2,gamma = 3,node_hidden=dim,rel_hidden=dim,\n",
    "                                      triple_size = triple_size,batch_size = batch_size)\n",
    "    #model.summary()\n",
    "    losses,ave_loss =[],[]\n",
    "    batch = get_train_set(train_pair,ents1,ents2,batch_size=batch_size)\n",
    "    for epoch in range(2000):\n",
    "        train_set = next(batch)\n",
    "        inputs = [adj_matrix,rel_matrix,ent_matrix,train_set]\n",
    "        inputs = [np.expand_dims(item,axis=0) for item in inputs]\n",
    "\n",
    "        loss = model.train_on_batch(inputs, np.array([0]))\n",
    "        losses.append(loss)\n",
    "        ave_loss.append(np.mean(losses[-100:]))\n",
    "        \n",
    "        if epoch%100 == 0:\n",
    "            print('\\tEpoch %d \\tloss=%.5f \\tave_loss=%.4f'%(epoch,loss,ave_loss[-1]))\n",
    "\n",
    "        if epoch > 500 and ave_loss[-1] > ave_loss[-100] and loss<ave_loss[-1]:\n",
    "            print('\\tEpoch %d \\tloss=%.5f \\tave_loss=%.4f'%(epoch,loss,ave_loss[-1]))\n",
    "            break # Early stop.\n",
    "            \n",
    "    vec = get_emb.predict_on_batch(inputs[:-1])            \n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pairs(vec,anchors,ents1,ents2,a0,q):\n",
    "    new_pair = set()    \n",
    "    re_ents1 = sorted(list(ents1 - set(anchors[:,0])))\n",
    "    re_ents2 = sorted(list(ents2 - set(anchors[:,1])))\n",
    "    \n",
    "    Lvec = np.array([vec[i] for i in re_ents1])\n",
    "    Rvec = np.array([vec[i] for i in re_ents2])\n",
    "    \n",
    "    Lvec = Lvec / np.linalg.norm(Lvec,axis=-1,keepdims=True)\n",
    "    Rvec = Rvec / np.linalg.norm(Rvec,axis=-1,keepdims=True)\n",
    "    sim_o = -Lvec.dot(Rvec.T)\n",
    "    Lsim = sim_o.argsort(-1)\n",
    "    Rsim = sim_o.argsort(0)\n",
    "\n",
    "    c1,c2=0,0\n",
    "    for i,j in enumerate(Lsim[:,0]):\n",
    "        dist = -sim_o[i,j]\n",
    "        if Rsim[0,j]==i:\n",
    "            c1+=1\n",
    "            thresh = max(a0 - n_iter*q,0.8)\n",
    "            if dist > thresh:\n",
    "                c2+=1\n",
    "                e1,e2 = re_ents1[i],re_ents2[j]\n",
    "                pair = str([e1,e2])\n",
    "                new_pair.add(pair)\n",
    "    #print('\\tPotential pairs:',len(re_ents1),c1,c2,'\\tThreshold:',thresh)\n",
    "    return new_pair\n",
    "\n",
    "def get_train_set(train_pair,ents1,ents2,batch_size):\n",
    "    train = np.repeat(train_pair,batch_size//len(train_pair)+1,axis=0)\n",
    "    np.random.shuffle(train); train = train[:batch_size]\n",
    "    while True:\n",
    "        f = np.random.randint(0,node_size,train.shape)\n",
    "        train_set = np.concatenate([train,f],axis = -1)\n",
    "        yield train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Dec 29 22:29:45 2020 \tLoading data...\n",
      "-----------------------------------------------------------------------------------------------\n",
      "Tue Dec 29 22:29:55 2020 \tSeed:0 Iter: 0, Dataset: zh_en/, # of train pairs: 4500\n",
      "\tRunning the structure model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yons/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/gradients_impl.py:110: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tEpoch 0 \tloss=5.41147 \tave_loss=5.4115\n",
      "\tEpoch 100 \tloss=0.00669 \tave_loss=0.2032\n",
      "\tEpoch 200 \tloss=0.00529 \tave_loss=0.0066\n",
      "\tEpoch 300 \tloss=0.00337 \tave_loss=0.0046\n",
      "\tEpoch 400 \tloss=0.00326 \tave_loss=0.0036\n",
      "\tEpoch 500 \tloss=0.00269 \tave_loss=0.0030\n",
      "\tEpoch 600 \tloss=0.00200 \tave_loss=0.0026\n",
      "\tEpoch 700 \tloss=0.00302 \tave_loss=0.0024\n",
      "\tEpoch 800 \tloss=0.00260 \tave_loss=0.0022\n",
      "\tEpoch 900 \tloss=0.00164 \tave_loss=0.0021\n",
      "\tEpoch 1000 \tloss=0.00229 \tave_loss=0.0019\n",
      "\tEpoch 1026 \tloss=0.00059 \tave_loss=0.0020\n",
      "\tScore:  [59.32380952380952, 88.3047619047619, 0.694690007435666, 56.5047619047619, 86.52380952380952, 0.6691877775592217]\n",
      "\tStruc model found 5171 new pairs in Iter 0\n",
      "\tRunning the attribute model...\n",
      "\tEpoch 0 \tloss=2.66541 \tave_loss=2.6654\n",
      "\tEpoch 100 \tloss=0.04206 \tave_loss=0.1136\n",
      "\tEpoch 200 \tloss=0.02853 \tave_loss=0.0340\n",
      "\tEpoch 300 \tloss=0.02181 \tave_loss=0.0261\n",
      "\tEpoch 400 \tloss=0.01999 \tave_loss=0.0221\n",
      "\tEpoch 500 \tloss=0.02006 \tave_loss=0.0197\n",
      "\tEpoch 600 \tloss=0.01721 \tave_loss=0.0183\n",
      "\tEpoch 700 \tloss=0.01560 \tave_loss=0.0167\n",
      "\tEpoch 791 \tloss=0.01547 \tave_loss=0.0167\n",
      "\tScore: [43.819047619047616, 70.15238095238095, 0.5289011996474038, 43.838095238095235, 69.5047619047619, 0.526708840679706]\n",
      "\tAttr model found 1195 new pairs in Iter 0\n",
      "-----------------------------------------------------------------------------------------------\n",
      "Tue Dec 29 22:35:05 2020 \tSeed:0 Iter: 1, Dataset: zh_en/, # of train pairs: 10866\n",
      "\tRunning the structure model...\n",
      "\tEpoch 0 \tloss=5.56831 \tave_loss=5.5683\n",
      "\tEpoch 100 \tloss=0.00855 \tave_loss=0.2121\n",
      "\tEpoch 200 \tloss=0.00527 \tave_loss=0.0072\n",
      "\tEpoch 300 \tloss=0.00406 \tave_loss=0.0051\n",
      "\tEpoch 400 \tloss=0.00369 \tave_loss=0.0041\n",
      "\tEpoch 500 \tloss=0.00376 \tave_loss=0.0034\n",
      "\tEpoch 600 \tloss=0.00281 \tave_loss=0.0030\n",
      "\tEpoch 700 \tloss=0.00225 \tave_loss=0.0028\n",
      "\tEpoch 800 \tloss=0.00276 \tave_loss=0.0025\n",
      "\tEpoch 900 \tloss=0.00161 \tave_loss=0.0023\n",
      "\tEpoch 1000 \tloss=0.00308 \tave_loss=0.0022\n",
      "\tEpoch 1030 \tloss=0.00144 \tave_loss=0.0022\n",
      "\tScore:  [65.26666666666667, 90.72380952380954, 0.7427882717654769, 63.88571428571429, 89.56190476190477, 0.729214619307281]\n",
      "\tStruc model found 3281 new pairs in Iter 1\n",
      "\tRunning the attribute model...\n",
      "\tEpoch 0 \tloss=1.76562 \tave_loss=1.7656\n",
      "\tEpoch 100 \tloss=0.04453 \tave_loss=0.1117\n",
      "\tEpoch 200 \tloss=0.03546 \tave_loss=0.0376\n",
      "\tEpoch 300 \tloss=0.02994 \tave_loss=0.0301\n",
      "\tEpoch 400 \tloss=0.02194 \tave_loss=0.0259\n",
      "\tEpoch 500 \tloss=0.02144 \tave_loss=0.0238\n",
      "\tEpoch 600 \tloss=0.02355 \tave_loss=0.0213\n",
      "\tEpoch 700 \tloss=0.02210 \tave_loss=0.0204\n",
      "\tEpoch 800 \tloss=0.01899 \tave_loss=0.0194\n",
      "\tEpoch 900 \tloss=0.01940 \tave_loss=0.0186\n",
      "\tEpoch 1000 \tloss=0.01545 \tave_loss=0.0177\n",
      "\tEpoch 1100 \tloss=0.02062 \tave_loss=0.0171\n",
      "\tEpoch 1174 \tloss=0.01353 \tave_loss=0.0171\n",
      "\tScore: [53.84761904761904, 78.71428571428571, 0.6268122321981635, 52.97142857142857, 78.50476190476189, 0.6195740624082082]\n",
      "\tAttr model found 915 new pairs in Iter 1\n",
      "-----------------------------------------------------------------------------------------------\n",
      "Tue Dec 29 22:40:22 2020 \tSeed:0 Iter: 2, Dataset: zh_en/, # of train pairs: 15062\n",
      "\tRunning the structure model...\n",
      "\tEpoch 0 \tloss=5.69477 \tave_loss=5.6948\n",
      "\tEpoch 100 \tloss=0.01114 \tave_loss=0.2250\n",
      "\tEpoch 200 \tloss=0.00641 \tave_loss=0.0082\n",
      "\tEpoch 300 \tloss=0.00507 \tave_loss=0.0059\n",
      "\tEpoch 400 \tloss=0.00496 \tave_loss=0.0048\n",
      "\tEpoch 500 \tloss=0.00459 \tave_loss=0.0039\n",
      "\tEpoch 600 \tloss=0.00397 \tave_loss=0.0035\n",
      "\tEpoch 700 \tloss=0.00449 \tave_loss=0.0031\n",
      "\tEpoch 800 \tloss=0.00257 \tave_loss=0.0028\n",
      "\tEpoch 900 \tloss=0.00289 \tave_loss=0.0027\n",
      "\tEpoch 1000 \tloss=0.00184 \tave_loss=0.0025\n",
      "\tEpoch 1071 \tloss=0.00240 \tave_loss=0.0024\n",
      "\tScore:  [69.21904761904763, 91.66666666666666, 0.7714731287837492, 68.01904761904763, 90.6952380952381, 0.759972857480092]\n",
      "\tStruc model found 1388 new pairs in Iter 2\n",
      "\tRunning the attribute model...\n",
      "\tEpoch 0 \tloss=2.06223 \tave_loss=2.0622\n",
      "\tEpoch 100 \tloss=0.05262 \tave_loss=0.1218\n",
      "\tEpoch 200 \tloss=0.03378 \tave_loss=0.0424\n",
      "\tEpoch 300 \tloss=0.03180 \tave_loss=0.0340\n",
      "\tEpoch 400 \tloss=0.02890 \tave_loss=0.0286\n",
      "\tEpoch 500 \tloss=0.02480 \tave_loss=0.0261\n",
      "\tEpoch 600 \tloss=0.02202 \tave_loss=0.0242\n",
      "\tEpoch 700 \tloss=0.02277 \tave_loss=0.0232\n",
      "\tEpoch 800 \tloss=0.02064 \tave_loss=0.0223\n",
      "\tEpoch 900 \tloss=0.02055 \tave_loss=0.0207\n",
      "\tEpoch 1000 \tloss=0.02395 \tave_loss=0.0199\n",
      "\tEpoch 1100 \tloss=0.01848 \tave_loss=0.0194\n",
      "\tEpoch 1200 \tloss=0.01905 \tave_loss=0.0188\n",
      "\tEpoch 1300 \tloss=0.01789 \tave_loss=0.0185\n",
      "\tEpoch 1400 \tloss=0.01883 \tave_loss=0.0173\n",
      "\tEpoch 1500 \tloss=0.01699 \tave_loss=0.0168\n",
      "\tEpoch 1600 \tloss=0.01521 \tave_loss=0.0166\n",
      "\tEpoch 1633 \tloss=0.01560 \tave_loss=0.0166\n",
      "\tScore: [57.15238095238095, 82.28571428571428, 0.6607809166872802, 56.39999999999999, 81.67619047619048, 0.653605275949382]\n",
      "\tAttr model found 603 new pairs in Iter 2\n",
      "-----------------------------------------------------------------------------------------------\n",
      "Tue Dec 29 22:46:11 2020 \tSeed:0 Iter: 3, Dataset: zh_en/, # of train pairs: 17053\n",
      "\tRunning the structure model...\n",
      "\tEpoch 0 \tloss=5.76437 \tave_loss=5.7644\n",
      "\tEpoch 100 \tloss=0.01086 \tave_loss=0.2381\n",
      "\tEpoch 200 \tloss=0.00729 \tave_loss=0.0093\n",
      "\tEpoch 300 \tloss=0.00586 \tave_loss=0.0063\n",
      "\tEpoch 400 \tloss=0.00503 \tave_loss=0.0052\n",
      "\tEpoch 500 \tloss=0.00307 \tave_loss=0.0043\n",
      "\tEpoch 600 \tloss=0.00414 \tave_loss=0.0039\n",
      "\tEpoch 700 \tloss=0.00310 \tave_loss=0.0035\n",
      "\tEpoch 800 \tloss=0.00211 \tave_loss=0.0031\n",
      "\tEpoch 900 \tloss=0.00288 \tave_loss=0.0031\n",
      "\tEpoch 911 \tloss=0.00295 \tave_loss=0.0031\n",
      "\tScore:  [70.06666666666666, 91.12380952380953, 0.7762827076646315, 69.55238095238096, 90.53333333333333, 0.7718157599747292]\n",
      "\tStruc model found 459 new pairs in Iter 3\n",
      "\tRunning the attribute model...\n",
      "\tEpoch 0 \tloss=2.32853 \tave_loss=2.3285\n",
      "\tEpoch 100 \tloss=0.06127 \tave_loss=0.1315\n",
      "\tEpoch 200 \tloss=0.03890 \tave_loss=0.0466\n",
      "\tEpoch 300 \tloss=0.03078 \tave_loss=0.0359\n",
      "\tEpoch 400 \tloss=0.02604 \tave_loss=0.0307\n",
      "\tEpoch 500 \tloss=0.02703 \tave_loss=0.0280\n",
      "\tEpoch 600 \tloss=0.02638 \tave_loss=0.0261\n",
      "\tEpoch 700 \tloss=0.02311 \tave_loss=0.0245\n",
      "\tEpoch 800 \tloss=0.02699 \tave_loss=0.0231\n",
      "\tEpoch 900 \tloss=0.02325 \tave_loss=0.0223\n",
      "\tEpoch 1000 \tloss=0.02038 \tave_loss=0.0215\n",
      "\tEpoch 1100 \tloss=0.02280 \tave_loss=0.0211\n",
      "\tEpoch 1200 \tloss=0.02012 \tave_loss=0.0199\n",
      "\tEpoch 1300 \tloss=0.02330 \tave_loss=0.0193\n",
      "\tEpoch 1400 \tloss=0.01871 \tave_loss=0.0191\n",
      "\tEpoch 1408 \tloss=0.01769 \tave_loss=0.0193\n",
      "\tScore: [56.971428571428575, 82.98095238095237, 0.6614373859612802, 55.647619047619045, 82.42857142857143, 0.6509770041634344]\n",
      "\tAttr model found 439 new pairs in Iter 3\n",
      "-----------------------------------------------------------------------------------------------\n",
      "Tue Dec 29 22:51:11 2020 \tSeed:0 Iter: 4, Dataset: zh_en/, # of train pairs: 17951\n",
      "\tRunning the structure model...\n",
      "\tEpoch 0 \tloss=5.79163 \tave_loss=5.7916\n",
      "\tEpoch 100 \tloss=0.01232 \tave_loss=0.2427\n",
      "\tEpoch 200 \tloss=0.00832 \tave_loss=0.0099\n",
      "\tEpoch 300 \tloss=0.00576 \tave_loss=0.0070\n",
      "\tEpoch 400 \tloss=0.00484 \tave_loss=0.0054\n",
      "\tEpoch 500 \tloss=0.00357 \tave_loss=0.0046\n",
      "\tEpoch 600 \tloss=0.00372 \tave_loss=0.0042\n",
      "\tEpoch 700 \tloss=0.00341 \tave_loss=0.0037\n",
      "\tEpoch 800 \tloss=0.00339 \tave_loss=0.0034\n",
      "\tEpoch 900 \tloss=0.00394 \tave_loss=0.0030\n",
      "\tEpoch 987 \tloss=0.00219 \tave_loss=0.0030\n",
      "\tScore:  [70.17142857142858, 90.66666666666666, 0.7763843446192779, 69.44761904761904, 90.15238095238095, 0.7703310594283131]\n",
      "\tStruc model found 138 new pairs in Iter 4\n",
      "\tRunning the attribute model...\n",
      "\tEpoch 0 \tloss=2.32072 \tave_loss=2.3207\n",
      "\tEpoch 100 \tloss=0.05755 \tave_loss=0.1366\n",
      "\tEpoch 200 \tloss=0.04235 \tave_loss=0.0491\n",
      "\tEpoch 300 \tloss=0.02897 \tave_loss=0.0379\n",
      "\tEpoch 400 \tloss=0.03724 \tave_loss=0.0331\n",
      "\tEpoch 500 \tloss=0.02639 \tave_loss=0.0300\n",
      "\tEpoch 600 \tloss=0.02308 \tave_loss=0.0279\n",
      "\tEpoch 700 \tloss=0.01888 \tave_loss=0.0259\n",
      "\tEpoch 800 \tloss=0.02673 \tave_loss=0.0246\n",
      "\tEpoch 900 \tloss=0.02355 \tave_loss=0.0235\n",
      "\tEpoch 1000 \tloss=0.01632 \tave_loss=0.0225\n",
      "\tEpoch 1100 \tloss=0.01822 \tave_loss=0.0221\n",
      "\tEpoch 1200 \tloss=0.02423 \tave_loss=0.0211\n",
      "\tEpoch 1300 \tloss=0.01950 \tave_loss=0.0208\n",
      "\tEpoch 1400 \tloss=0.02326 \tave_loss=0.0203\n",
      "\tEpoch 1500 \tloss=0.01547 \tave_loss=0.0195\n",
      "\tEpoch 1600 \tloss=0.01525 \tave_loss=0.0193\n",
      "\tEpoch 1700 \tloss=0.02012 \tave_loss=0.0190\n",
      "\tEpoch 1800 \tloss=0.01643 \tave_loss=0.0184\n",
      "\tEpoch 1900 \tloss=0.01611 \tave_loss=0.0178\n",
      "\tScore: [57.19047619047619, 83.39999999999999, 0.6661366598913423, 56.36190476190476, 82.75238095238096, 0.658004461992503]\n",
      "\tAttr model found 255 new pairs in Iter 4\n",
      "Early stop...\n"
     ]
    }
   ],
   "source": [
    "n_train = 0.3\n",
    "dim = 100\n",
    "for dataset in ['zh_en/','ja_en/','fr_en/']:\n",
    "    for seed in range(5):\n",
    "        result=[]\n",
    "        train_pair,test_pair = load_anchor('data/'+dataset, train_ratio = n_train,seed=seed)\n",
    "        adj_matrix,adj_features,rel_features,ent_prop,ents1,ents2 =load_data('data/'+dataset,p=0.0001)\n",
    "        adj_matrix = np.stack(adj_matrix.nonzero(),axis = 1)\n",
    "        rel_matrix = np.stack(rel_features.nonzero(),axis = 1)\n",
    "        ent_matrix = np.stack(adj_features.nonzero(),axis = 1)\n",
    "        ent_prop = ent_prop.todense()\n",
    "        prop_size = ent_prop.shape[1]\n",
    "        node_size = adj_features.shape[1]\n",
    "        rel_size = rel_features.shape[1]\n",
    "        triple_size = len(adj_matrix)\n",
    "        batch_size = node_size\n",
    "        \n",
    "        score_list=[]\n",
    "        for n_iter in range(10):\n",
    "            print('-----------------------------------------------------------------------------------------------')    \n",
    "            print(time.ctime(),'\\tSeed:%d Iter: %d, Dataset: %s, # of train pairs: %d'%(seed,n_iter,dataset,len(train_pair)))\n",
    "\n",
    "            # Struture model\n",
    "            K.clear_session()\n",
    "            print('\\tRunning the structure model...')\n",
    "            embeddings = model_struc(adj_matrix,rel_matrix,ent_matrix,train_pair,dim=dim,lr=0.005)\n",
    "            score = get_hits(embeddings,test_pair)\n",
    "            struc_model_score = score[-1]\n",
    "            record = ['Sturc model',dataset,seed,n_iter,n_train,dim]+score\n",
    "            result.append(record)\n",
    "            print('\\tScore: ',score)\n",
    "            \n",
    "            pairs = get_pairs(embeddings,train_pair,ents1,ents2,a0=0.9,q=0.05)\n",
    "            new_pair = np.array([eval(i) for i in pairs])\n",
    "            train_pair = np.vstack((train_pair,new_pair))\n",
    "            print('\\tStruc model found %d new pairs in Iter %d'%(len(new_pair),n_iter))\n",
    "            \n",
    "            # Attribute model\n",
    "            ent_prop_matrix = extension_attr(ent_prop,train_pair)\n",
    "            ent_prop_matrix = adj_features.dot(ent_prop_matrix)\n",
    "            \n",
    "            K.clear_session()\n",
    "            print('\\tRunning the attribute model...')\n",
    "            embeddings = model_attr(ent_prop_matrix,train_pair,dim=dim,lr=0.005)\n",
    "            score = get_hits(embeddings,test_pair)\n",
    "            attr_model_score = score[-1]\n",
    "            record = ['Attr model',dataset,seed,n_iter,n_train,dim]+score\n",
    "            result.append(record)\n",
    "            print('\\tScore:',score)\n",
    "            \n",
    "            pairs = get_pairs(embeddings,train_pair,ents1,ents2,a0=0.95,q=0.05)\n",
    "            new_pair = np.array([eval(i) for i in pairs])\n",
    "            train_pair = np.vstack((train_pair,new_pair))\n",
    "            print('\\tAttr model found %d new pairs in Iter %d'%(len(new_pair),n_iter))   \n",
    "            \n",
    "            # Early stop\n",
    "            score_list.append(max(attr_model_score,struc_model_score))\n",
    "            if n_iter>3 and score_list[-1]-score_list[-2]<0.1:\n",
    "                print('Early stop...')\n",
    "                break\n",
    "                \n",
    "        json.dump(eval(str(result)),open('result_dataset_{}_seed_{}.txt'.format(dataset[:-1],seed),'w'))\n",
    "        break # Uncomment this line to repeat evaluation on other random seeds.\n",
    "    break # Uncomment this line to evaluate on other datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
